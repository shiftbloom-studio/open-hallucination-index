Open Hallucination Index: Ein notwendiges epistemisches Korrektiv im Zeitalter probabilistischer generativer KI
1. Einführung: Die Krise der synthetischen Wahrheit

Probabilistische Large Language Models (LLMs) haben in den letzten Jahren enorme Fortschritte in der Inferenz und Textgenerierung erzielt, leiden jedoch an einem grundlegenden epistemischen Defizit: Sie halluzinieren häufig Fakten und präsentieren synthetische Wahrheit – plausible, aber unbelegte Aussagen
arxiv.org
. Bender et al. prägten für solche Modelle den Begriff „stochastische Papageien“, da LLMs aus statistischer Probabilistik heraus Wortsequenzen zusammensetzen, ohne Referenz auf Bedeutung oder Realitätsgehalt
en.wikipedia.org
en.wikipedia.org
. Diese Halluzinationsneigung stellt eine Krise der Verlässlichkeit dar, wenn generative KI ungeprüft als Wissensquelle dient. So produzierte z. B. GPT-3.5 in einer Studie weniger als 25 % faktisch korrekte Aussagen
arxiv.org
 – ein ernüchternder Befund, der deutlich macht, dass wir dringend unabhängige Faktenprüfungsinstanzen benötigen, um generierten Text zu verifizieren.

Ansätze wie Retrieval-Augmented Generation (RAG) versuchen, LLM-Antworten durch vorheriges Heranziehen von Dokumenten zu fundieren. Doch RAG verlässt sich immer noch auf das LLM bei der Auswahl und Interpretation der Quellen. Wenn das Modell selbst halluziniert, welche Quellen relevant sind oder falsch aus dem Kontext zitiert, greift RAG zu kurz. Mit anderen Worten: Die Integrität der Antwort bleibt gefährdet, solange die Prüfung der Quellen in der Domäne desselben generativen Modells liegt, das auch die Antwort formuliert hat. Deterministische Faktenprüfung muss von der generativen Komponente entkoppelt werden, um einer durch Stochastik geprägten KI-Ausgabe objektiv begegnen zu können.

Der Open Hallucination Index (OHI) verfolgt genau dieses Ziel als ontologisches „Trust Layer“ für KI. Statt OHI als Produktfeature zu betrachten, begreifen wir es als notwendiges epistemisches Korrektiv: eine Zwischenschicht der Wahrheit, die zwischen generativem Modell und Nutzer geschaltet ist. OHI übernimmt die Bonitätsprüfung von Fakten nach der Texteerzeugung. Ein vom OHI inspiriertes Motto lautet „LLMs Hallucinate – We Verify“, was die Rollenverteilung verdeutlicht: Das LLM generiert Inhalte, OHI verifiziert die behaupteten Fakten unabhängig. Diese Trennung von Inferenz und Verifikation ist ontologisch fundiert – OHI stützt sich auf strukturierte, externe Wissensquellen (etwa Wissensgraphen, Dokumentationsdatenbanken), um jeden behaupteten Sachverhalt zu prüfen. Dadurch entsteht ein epistemologisches Instrument, das die probabilistische Textinferenz der LLMs von einem deterministischen Faktenabgleich entkoppelt. OHI fungiert als „Trust Layer for AI“, welches die Aussagen der KI nachträglich mit Wissensbasen abgleicht und einen Vertrauenswert liefert, ohne den kreativen Fluss der Generierung einzuschränken. Im Ergebnis verschiebt sich der Paradigmenfokus von „Generative AI“ hin zu „Verifiable AI“, in der nicht allein die Kohärenz des Outputs zählt, sondern seine nachweisbare Wahrheit.

2. Theoretischer Ansatz: Claim Decomposition & GraphRAG

Um KI-generierte Texte systematisch prüfen zu können, zerlegt OHI komplexe Aussagen mittels Claim Decomposition in atomare Behauptungen. Jeder zusammengesetzte Satz oder Absatz wird in einzelne Subjekt-Prädikat-Objekt-Aussagen aufgespalten, die isoliert überprüfbar sind. Diese Aufteilung in elementare Fakten ist entscheidend, da nur ausreichend feingranulare Claims exakt gegen eine Wissensbasis gematcht werden können. Beispielsweise würde ein Satz wie „Albert Einstein, der 1879 in Ulm geboren wurde, erhielt 1921 den Nobelpreis für Physik“ in zwei Teilbehauptungen zerlegt: (1) Einstein wurde 1879 in Ulm geboren. (2) Einstein erhielt 1921 den Physik-Nobelpreis. Jede dieser Aussagen kann getrennt verifiziert oder falsifiziert werden. Für die Claim-Decomposition nutzt OHI ein sprachliches Modell (ein lokal laufendes LLM), das den Eingabetext nach impliziten Einzelaussagen scannt und diese formal extrahiert. Durch diese Ontologisierung der generierten Inhalte – also ihre Überführung in eine strukturierte, an einen Wissensgraph angelehnte Form – schafft OHI die Voraussetzung für deterministische Faktenabgleiche.

Der zweite Pfeiler des theoretischen Ansatzes ist GraphRAG, eine erweiterte Form von Retrieval-Augmentation, die Knowledge Graph Retrieval mit Vektor-Suche kombiniert. Hierbei dient ein Neo4j-Wissensgraph aus Wikipedia-Daten als hochpräziser Index für Fakten, während ein Vektorraum-Index (Qdrant) als semantische Ergänzung fungiert. Die Datenarchitektur des OHI importiert einen vollständigen Wikipedia-Dump in den Graphen (siehe import_wikipedia_to_neo4j.py). Jeder Wikipedia-Artikel wird zu einem Knoten (:Article) mit Eigenschaften (Titel, Text, etc.), kategorisiert und verlinkt mit anderen Artikeln. So entstehen ontologische Relationen: Artikelknoten werden über IN_CATEGORY-Kanten mit ihren Kategorien verknüpft, und Hyperlinks im Fließtext werden als LINKS_TO-Beziehungen zwischen Artikelknoten abgebildet. Sogar Weiterleitungen (Redirects) werden durch spezielle Kanten modelliert. Diese Graphstruktur verleiht dem Wissensindex eine hohe semantische Dichte – jede Kante trägt eine eindeutig interpretierbare Bedeutung (z. B. “liegt in Kategorie XY” oder “verweist auf Artikel Z”), was eine deterministische Inferenz über Fakten erlaubt. So kann der Graph z. B. präzise beantworten, ob ein gegebener Artikel explizit eine Beziehung zu einem anderen Begriff aufweist (etwa ob Person X in Stadt Y geboren ist, könnte durch eine Kategorie “Geboren in Y” oder einen Link zu Y angezeigt werden). Diese strukturierte Semantik erlaubt auch transitive Schlussfolgerungen: Liegen zwei Faktenketten im Graph nahe beieinander, kann OHI Relationen über mehrere Hops verfolgen (etwa Zugehörigkeiten über Kategorienhierarchien).

Dem gegenüber steht der Vektorraum-Index (Qdrant), der Wikipedia-Wissen in kontinuierlicher Form repräsentiert. Hier werden Textembeddings (z. B. Absätze oder Zusammenfassungen) in einem hochdimensionalen Raum abgelegt, wobei semantisch ähnliche Inhalte in Nachbarschaft liegen. Die semantische Ähnlichkeitssuche kann kontextuell passende Wissensfragmente finden, auch wenn keine explizite Graph-Kante existiert. Allerdings ist die Relevanzmetrik hier probabilistisch und basiert auf numerischen Ähnlichkeitsscores, was weniger zielgerichtet ist als die binären Relationen des Graphen. Präzision vs. Abdeckung beschreibt das Zusammenspiel der beiden Ansätze: Der Graph liefert hohe Präzision (geringe Fehlertoleranz, nahezu kein false positive, da eine Kante entweder vorhanden ist oder nicht), während der Vektorraum eine höhere Abdeckung bietet, indem er auch implizit verwandte Inhalte entdeckt. In puncto Inferenz bedeutet dies: Der Graph kann garantiert richtige Beziehungen finden, verfehlt aber evtl. Antworten, wenn Wissen nicht exakt als Kante vorliegt; der Vektorraum findet mit weicher Logik auch indirekt Passendes, riskiert aber semantisches Rauschen (falsch-positive Matches). OHI adressiert dies durch Kombination beider Modi (siehe Abschnitt 3): GraphRAG holt sich Fakten aus dem Neo4j-Graphen (deterministisch) und relevante Passagen aus dem Vektorindex (probabilistisch). Durch diese epistemische Redundanz – eine Mehrfachabsicherung derselben Behauptung über unabhängige Wissenskanäle – erhöht sich die Zuverlässigkeit der Verifikation: Stimmen Graph-Hinweise und semantische Textähnlichkeit überein, untermauert dies die faktische Wahrheit umso stärker. Transitivität und logische Schlussfolgerungen können so teils über Graphwege, teils über semantische Textbrücken erfolgen, was insgesamt eine robustere Faktenprüfung gewährleistet als jeder Ansatz für sich allein.

3. Implementierung als Beweis (Proof of Implementation)

Die konzeptionellen Ideen des OHI wurden in einem ausführlichen Prototyping-Stack umgesetzt, um ihre Machbarkeit zu demonstrieren. Ein zentrales Element ist die Standardisierung externer Wissensabfragen durch das Model Context Protocol (MCP). Über MCP werden externe Datenquellen wie Wikipedia und Context7 (eine API für Programm-Dokumentationen) als Wahrheits-Orakel angebunden, ohne dass das LLM selbst frei im Internet suchen muss. Technisch realisiert dies ein MCP-Adapter (WikipediaMCPAdapter in mcp_wikipedia.py), der als Sidecar-Service läuft. Dieser Adapter öffnet eine persistente Verbindung zu einem lokalen Wikipedia-MCP-Server über Server-Sent Events (SSE) und hält diese Verbindung offen für mehrere Anfragen. Im Code wird dazu ein sse_client genutzt, der einen kontinuierlichen Event-Stream aufbaut. Um Latenzen zu minimieren, implementiert der Adapter Session-Pooling: Beim Start werden bis zu 3 parallele MCP-Sessions initialisiert, die in einem Pool verwaltet werden. Diese MCPSessionPool erlaubt es, wiederholte Abfragen (z. B. mehrere Suchanfragen hintereinander) asynchron auszuführen, ohne für jede Anfrage eine neue TCP-Verbindung aufzubauen. Die Logik ist so gestaltet, dass wenn der Pool gesund ist, stets eine bereits offene Session genutzt wird; andernfalls fällt der Adapter auf eine Einmal-Session zurück. Durch diese persistenten SSE-Verbindungen können auch Streams von Ergebnissen empfangen werden – beispielsweise liefert der Context7-Adapter Ergebnisse über einen HTTP-Stream, was bei umfangreichen Dokumentationstexten von Vorteil ist. Beide Adapter (Wikipedia und Context7) verwenden asynchrone Programmierung und erlauben parallele Schritte: Der Wikipedia-Adapter z. B. startet eine Suche und ruft dann parallel die Zusammenfassungen der Top-3 Treffer ab, wie durch asyncio.gather umgesetzt (siehe find_evidence in mcp_wikipedia.py). So werden mehrere HTTP-Abfragen concurrent ausgeführt, was die Gesamtlaufzeit senkt. Jede gefundene Evidenz wird standardisiert in eine Evidence-Datenstruktur gegossen, die neben dem Text auch Meta-Informationen wie Quelle, Zeitstempel und einen initialen Konfidenzscore enthält – im Fall der Wikipedia-Zusammenfassung beispielsweise einen heuristischen Ähnlichkeitsscore von 0.85. Diese Standardisierung aller Wissenszugriffe via MCP schafft Reproduzierbarkeit: Unabhängig davon, welches LLM geprüft wird, greift OHI stets auf dieselben festgelegten Tools (z. B. search_wikipedia, get_summary) und denselben Datenbestand zurück. Durch die Abstraktion über MCP bleibt die Integrität der Faktenprüfung gewährleistet und ist nicht vom internen Zustand des generativen Modells abhängig.

Ein weiterer Schwerpunkt der Implementierung ist die „lokale Souveränität“ der gesamten Pipeline. Sämtliche Komponenten laufen in einem abgeschotteten lokalen Docker-Netzwerk, wodurch keine Abhängigkeit von externen Closed-Source-Diensten besteht. Für die Sprachverarbeitung kommt ein lokal gehostetes LLM zum Einsatz: Das Modell Qwen2.5-7B-Instruct (ein 7-Milliarden-Parameter Modell) wird über den Hochleistungs-Inferenzserver vLLM bereitgestellt. Die Infrastruktur (docker-compose.yml) zeigt, dass das OHI-API-Backend über LLM_BASE_URL: "http://vllm:8000/v1" mit dem lokalen Modell kommuniziert und keine API-Schlüssel für OpenAI o. Ä. benötigt. Auch die Embeddings für den Vektorindex werden vor Ort generiert, mittels Sentence-Transformer all-MiniLM-L6-v2. Dies bedeutet, dass weder Anfragen an externe Embedding-Services noch an proprietäre KI-Modelle notwendig sind. Die Entkopplung von Closed-Source-LLMs ist für eine neutrale Vertrauensmessung entscheidend: Würde man z. B. OpenAI’s eigenes Modell nutzen, um die Fakten von ChatGPT zu verifizieren, liefe man Gefahr, dieselben Trainingsbiases oder Halluzinationsmuster erneut zu erhalten – quasi „den Bock zum Gärtner machen“. OHI umgeht dieses Problem, indem ein unabhängiges, transparentes Modell verwendet wird, dessen Parametrisierung und Wissensstand kontrollierbar sind. Durch diese ontologische Neutralität ist die Verifikationsinstanz nicht von den Herstellungen desselben generativen Modells beeinflusst. Darüber hinaus ermöglicht die lokale Ausführung Datensouveränität: Sensible Inhalte verbleiben im eigenen System während der Prüfung, was in Domänen wie Medizin oder Recht essentiell sein kann. Die Docker-Compose-Definition spiegelt diese Philosophie der lokalen Autarkie: Neben dem LLM-Container laufen der Neo4j-Graph, Qdrant vektor DB, Redis-Cache und die MCP-Server (für Wikipedia und Context7) sämtlich on-premise. Dadurch sind die Antwortzeiten kalkulierbar und es bestehen keine externen Abhängigkeiten, die die Konsistenz der Ergebnisse gefährden könnten.

Die Kernprüfung erfolgt in der Klasse HybridVerificationOracle (implementiert im Modul dependencies.py), welche als Beweis der Integration dient. Dieses Modul verbindet alle vorbereiteten Komponenten – Graph-Store, Vektor-Store, MCP-Quellen – zu einer einheitlichen Prüf-Logik. Der Begriff Hybrid deutet bereits an, dass hier mehrere Verifikationsstrategien algorithmisch vereint werden. Tatsächlich unterstützt OHI verschiedene Modi, konfigurierbar über VERIFY_DEFAULT_STRATEGY: Etwa rein graphbasierte Verifikation (GRAPH_EXACT), rein vektorbasierte (VECTOR_SEMANTIC), eine gestufte Kaskade (CASCADING) oder den hybriden Ansatz (HYBRID). Standardmäßig ist der MCP-Enhanced-Modus aktiviert, welcher den hybriden Graph+Vektor-Ansatz noch um externe MCP-Evidenzen erweitert. Im Hybridmodus zerlegt das System zunächst den Eingabetext in Claims (wie oben beschrieben). Für jeden Claim führt das HybridVerificationOracle dann parallel zwei Prüfschritte durch: (1) Graph-Exact Match – es wird z. B. in Neo4j gesucht, ob ein Artikel-Knoten existiert, der die geforderte Beziehung aufweist (etwa ob Subjekt=„Einstein“ in Kategorie/Relation=„Nobelpreisträger 1921“ steht). (2) Vektor-Semantic Match – der Claim wird als Embedding in den Vektorraum projiziert und per k-NN-Suche nach inhaltlich ähnlichen Wikipedia-Passagen gesucht. Zusätzlich können (3) MCP-Quellen direkte Textevidenz liefern, z. B. eine zusammengefasste Beschreibung aus Wikipedia, die den Claim bestätigt oder widerlegt. Die Resultate all dieser Kanäle werden in Form von Evidence-Objekten mit Scores gesammelt. Anschließend kommt der WeightedScorer ins Spiel, der diese Evidenzscores zu einem einheitlichen Vertrauenswert aggregiert. Die Gewichtung erfolgt nach vordefinierten Regeln: So fließt etwa ein Graph-Treffer (der eine exakte ontologische Übereinstimmung darstellt) mit höherem Gewicht in den Score ein als ein bloß semantisch ähnlicher Textfund. Umgekehrt kann mehrere konsistente semantische Evidenz (etwa zwei verschiedene Artikel, die den Claim stützen) den Score ebenfalls stark anheben – hier zahlt sich die epistemische Redundanz aus. Intern könnte man sich das Scoring als Kombination zweier Skalen vorstellen: einer diskreten Wahrheitsskala (Graph: wahr/unbekannt) und einer kontinuierlichen Ähnlichkeitsskala (Vektor: 0–1), die der WeightedScorer zu einer Gesamtwahrscheinlichkeit der Gültigkeit verrechnet. Beispielsweise könnte der Oracle feststellen: „Claim X wird durch einen Graph-Fund gestützt (hohe Grundwahrscheinlichkeit) und zusätzlich durch einen semantisch ähnlichen Satz in Wikipedia bestätigt (weitere Erhöhung der Wahrscheinlichkeit). Ergebnis: Trust-Score 0.95.“ Um Robustheit zu gewährleisten, kann die Strategie auch als cascading eingestellt sein, d. h. es wird zuerst nach Graph-Belegen gesucht und nur bei Misserfolg auf Vektor-Suche ausgewichen – so werden false positives minimiert. Insgesamt demonstriert die Implementierung, dass die Hybrid-Oracle-Architektur praktisch funktioniert: Das Zusammenspiel aus strukturierter Ontologie (Neo4j) und unstrukturierter Semantik (Qdrant) sowie externer Wissensabfragen (MCP) liefert für beliebige Claims eine Menge an Evidenzen, aus der ein quantitativer Vertrauensindex berechnet werden kann. Dieser fließt schließlich in den VerifyTextUseCase, der die Pipeline abschließt und die Ergebnisse (inklusive relevanter Evidenzstellen aus Wikipedia/Context7) zurückmeldet.

4. Diskussion & Ausblick

OHI führt den abstrakten Begriff "Vertrauen" eines KI-Outputs auf einen konkreten, messbaren Score zurück. Der Trust-Score skaliert von 0.0 (kein Vertrauen bzw. keinerlei Evidenz) bis 1.0 (höchstes Vertrauen, Claim durch starke Evidenz belegt). Diese Quantifizierung ermöglicht es, generierten Text nicht mehr binär in wahr/falsch einzuteilen, sondern graduell nach Glaubwürdigkeit zu bewerten. Epistemologisch stellt dies den Versuch dar, den Grad an bestätigtem Wissen formal zu fassen – vergleichbar einer Wahrscheinlichkeit oder einem Confidence Level. Ein hoher Score indiziert, dass der generierte Inhalt redundant durch Wissensquellen gedeckt ist, während ein niedriger Score warnt, dass eine Aussage unbelegt oder zweifelhaft sein könnte. Allerdings sind die Grenzen dieser Quantifizierung zu beachten: Vertrauen ist multidimensional und kontextabhängig. Ein einzelner Score kann subtile Aspekte wie Quellenqualität, Aktualität des Wissens oder Widersprüche zwischen Quellen nicht vollständig ausdrücken. Die Wahl der Skala 0–1 suggeriert eine mathematische Präzision, die in der Praxis von Annahmen abhängt (z. B. Gewichtungen im WeightedScorer). Es besteht die Gefahr eines trügerischen Sicherheitsgefühls, wenn Nutzer einen numerisch hohen Score mit absoluter Wahrheit gleichsetzen. In Wirklichkeit spiegelt der Score nur die in OHI eingebetteten Wissensquellen wider – unbekannte Wahrheiten außerhalb von Wikipedia/Dokumentation bleiben unentdeckt. Dennoch bietet der Trust-Score einen wertvollen heuristischen Indikator, der die ansonsten qualitative Einschätzung der KI-Antwort quantifizierbar macht. In Anwendungsszenarien (etwa in der Medizin oder im Rechtswesen) könnte man Thresholds setzen: Antworten unter z. B. 0.7 Vertrauen werden verworfen oder bedürfen manueller Prüfung. Damit fungiert OHI als Frühwarnsystem gegen Halluzinationen, ohne menschliche Validierung völlig zu ersetzen.

Trotz der erfolgreichen Implementierung im Prototyp stehen Grenzen und Herausforderungen für OHI als produktives System zur Debatte:

Latenz & Performance: Die momentane Pipeline ist Python-dominiert, was die Ausführung zahlreicher gleichzeitiger asynchroner Tasks limitiert. Jeder Claim-Check involviert Datenbankzugriffe (Neo4j, Qdrant), Netzwerkkommunikation zwischen Containern und Inferenzaufrufe beim LLM – kumulativ entsteht pro überprüftem Satz eine spürbare Verzögerung. In Echtzeitanwendungen könnte dies problematisch sein. Ein Konzept zur Beschleunigung wäre eine Hybridimplementierung mit Rust für zeitkritische Pfade (z. B. Embedding-Berechnung oder parallele Vektor- und Graphqueries in nativer Geschwindigkeit). Qdrant und Neo4j selbst sind hochoptimiert (Rust bzw. Java), aber die Orchestrierung in Python ist ein Flaschenhals. Zukünftige Iterationen müssten die Parallelisierung noch effizienter nutzen (eventuell Batch-Verification mehrerer Claims gemeinsam) und überlegen, ob In-Memory-Optimierungen oder ein Graph-Datenbank-Embedding in einer Systemsprache die Gesamtperformance steigern können. Es gilt abzuwägen, inwieweit etwas höhere Latenz akzeptabel ist im Tausch für Transparenz; für viele kritische Anwendungen (Recherche, Wissenschaft) dürfte Verifikation vor Geschwindigkeit gehen, doch etwa in Dialogsystemen erwartet der Nutzer antwortnahe Reaktionszeiten.

Betriebskosten & Ressourcen: Das Vorhalten einer vollständigen Verifikationskette ist ressourcenintensiv. Ein lokales LLM mit mehreren Milliarden Parametern benötigt einen High-end-GPU-Server, die Neo4j-Instanz mit dem gesamten Wikipedia-Wissensgraph beansprucht signifikanten Speicher und laufende Pflege. Selbst mit AWQ-Quantisierung benötigt Qwen-7B laut Deployment ~24GB Grafikspeicher. Dazu kommen Speicherplatz für die Vektordatenbank (Wikipedia hat Millionen von Embeddings) und CPU/RAM für Datenbanken. Gegenüber einem einfachen API-Call an GPT ist dies ein komplexes System aus vielen Komponenten, was höhere Infrastrukturkosten bedeutet. Für große Organisationen mag dies tragbar sein, doch für kleine Anwendungen ist der Overhead beträchtlich. Ein möglicher Ausblick sind Cloud-optimierte Versionen oder skalierbare Cluster-Lösungen, welche die Last verteilen – allerdings bewegt man sich dann wieder weg von der reinen On-Premise-Souveränität. In jedem Fall muss die Wertschöpfung (vertrauenswürdige Antworten) den Mehraufwand rechtfertigen. In sicherheitskritischen Bereichen ist das oft gegeben, aber am Massenmarkt steht Aufwand und Nutzen noch in Relation.

Wartung der Ground-Truth: Ein Verifikationssystem ist nur so gut wie seine Wissensbasis. Die OHI-Pipeline stützt sich stark auf den Wikipedia-Dump und (optional) eine Programmdokumentations-API. Wikipedia wird laufend aktualisiert – OHI’s Graph müsste regelmäßig mit neuen Dumps geupdated oder per Streaming-Import synchron gehalten werden. Das Script import_wikipedia_to_neo4j.py bietet zwar eine Möglichkeit, den Import zu automatisieren (inkl. Resume-Funktion für Abbrüche), doch ein Delta-Update bestehender Graphen ist komplex. Ohne kontinuierliche Pflege droht die Ground-Truth-Erosion: Der Graph veraltet, neue Erkenntnisse fehlen, womit OHI irgendwann falsche Negative produzieren könnte (korrekte Aussagen erhalten einen niedrigen Score, weil der Wissensstand hinterherhinkt). Ähnliches gilt für die Vektordaten – neue Artikel oder veränderte Texte müssen nachembeddet werden. In der Praxis wäre hier eine Anbindung an Wikipedia’s Live-Feeds oder API denkbar, was aber den Aufbau eines nahezu in Echtzeit synchronisierten Graphen erfordern würde. Zudem ist Wikipedia umfassend, aber nicht unfehlbar; Qualitätskontrolle und Bias der Quellen sind weitere Aspekte. OHI selbst führt keine inhaltliche Plausibilitätsprüfung durch, es vertraut den eingebundenen Wissensquellen. Zukünftig könnte man erwägen, mehrere unabhängige Wissensquellen zu integrieren (z. B. Datenbanken, Publikationskorpora), um nicht vollständig auf Wikipedia als alleinige „Quelle der Wahrheit“ angewiesen zu sein. Damit steigt jedoch die Systemkomplexität weiter.

Zusammenfassend zeigt der Open Hallucination Index als Forschungsprototyp einen möglichen Paradigmenwechsel auf: weg von unbedarfter Generierung hin zu „Verifiable AI“. OHI erweitert generative KI um eine dringend notwendige Verifizierungsschicht, die für jedes ausgegebene Informationselement Rechenschaft einfordert. Es spiegelt gewissermaßen wissenschaftliche Praxis wider – Behauptungen brauchen Belege – und überträgt dieses Prinzip in automatisierter Form auf KI-Ausgaben. Während herkömmliche LLM-Systeme eloquent fabulieren, stellt OHI die Frage: „Woran machen wir fest, dass dies wahr ist?“ Indem es diese Frage mit konkreten Evidenzen aus Wissensgraph und Dokumenten beantwortet und einen Vertrauensindex liefert, wird ein Qualitätsmaßstab etabliert, an dem sich generierte Inhalte messen lassen müssen. Natürlich steckt der Ansatz noch in den Anfängen und wirft neue Fragen auf (z. B. nach der letztgültigen Autorität der Wahrheit, Performance-Grenzen, etc.), aber er markiert einen wichtigen Schritt. Im Fazit ist OHI nicht nur eine technische Implementation, sondern ein Konzept für die Zukunft der KI-Nutzung: Ein KI-System soll nicht mehr nur kreativ und kohärent sein, sondern auch prüfbar und überprüft – ein Partner, dem wir vertrauen können, weil er seine Aussagen mit Wissen begründen kann. Diese Umorientierung hin zu nachprüfbarer KI könnte langfristig die Akzeptanz und Zuverlässigkeit von KI-Systemen in gesellschaftlich sensiblen Bereichen entscheidend erhöhen.

Quellen:
https://arxiv.org/abs/2310.14564#:~:text=frequently%20%22hallucinate%2C%22%20resulting%20in%20non,outperforming%20more%20capable%20LLMs%20like
https://en.wikipedia.org/wiki/Stochastic_parrot#:~:text=In%20their%20paper%2C%20Bender%20et,5
https://en.wikipedia.org/wiki/Stochastic_parrot#:~:text=The%20tendency%20of%20LLMs%20to,11